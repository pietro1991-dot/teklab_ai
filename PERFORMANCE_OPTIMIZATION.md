# üöÄ Guida Ottimizzazione Prestazioni

## Problema: Generazione lenta

Se il chatbot impiega molto tempo a rispondere, ecco le soluzioni:

### ‚úÖ Soluzione 1: Ridurre lunghezza risposte
Nel file `chatbot_config.py`, modifica:
```python
MAX_NEW_TOKENS = 50  # Invece di 100-300
```
**Effetto**: Risposte pi√π brevi ma molto pi√π veloci (2-3x pi√π veloce)

### ‚úÖ Soluzione 2: Usare un modello pi√π piccolo
Scarica TinyLlama 1.1B (molto pi√π veloce):
```bash
python -c "from huggingface_hub import snapshot_download; snapshot_download('TinyLlama/TinyLlama-1.1B-Chat-v1.0', local_dir='ai_system/models/TinyLlama-1.1B-Chat')"
```

Poi modifica `chatbot_config.py`:
```python
MODEL_PATH = "ai_system/models/TinyLlama-1.1B-Chat"
```
**Effetto**: 5-10x pi√π veloce, usa solo 1GB VRAM

### ‚úÖ Soluzione 3: Ridurre chunk RAG
Nel file `chatbot_config.py`:
```python
RAG_TOP_K = 1  # Invece di 3
```
**Effetto**: Meno contesto ma prompt pi√π piccolo = generazione pi√π veloce

### üìä Benchmark GTX 1050 Ti (4GB):

| Modello | VRAM | Velocit√† | Qualit√† |
|---------|------|----------|---------|
| Llama 3.2 3B | 2.7GB | ~5 token/sec | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| TinyLlama 1.1B | 0.9GB | ~15 token/sec | ‚≠ê‚≠ê‚≠ê |

### üéØ Configurazione raccomandata per GTX 1050 Ti:

```python
# chatbot_config.py
MAX_NEW_TOKENS = 80  # Buon compromesso
TEMPERATURE = 0.8  # Pi√π creativo
RAG_TOP_K = 2  # 2 chunk invece di 3
```

**Tempo medio risposta**: 15-30 secondi per 80 token

### üí° Tips:
- Risposte di 50-100 token sono sufficienti per la maggior parte delle domande
- La prima generazione √® pi√π lenta (caricamento cache)
- Le generazioni successive sono pi√π veloci
- Su GPU 4GB, 3B √® il limite - evita modelli 7B+