# MULTI-USER QUEUE SYSTEM - Sistema di coda per backend Flask

Implementazione completa per gestire richieste concorrenti multipli utenti B2B.

## PROBLEMA

**Ollama √® single-threaded:** Pu√≤ processare 1 request alla volta.

**Scenario multi-utente:**
```
User A: Invia query ‚Üí Ollama genera (35s)
User B: Invia query ‚Üí ‚ùå BLOCKED (deve attendere)  
User C: Invia query ‚Üí ‚ùå BLOCKED (deve attendere)
```

**Senza queue:** User B e C ricevono timeout o errori.
**Con queue:** User B e C vedono "Posizione in coda: 2, 3" e attendono il loro turno.

## SOLUZIONE IMPLEMENTATA

### 1. RequestQueue Class

```python
class RequestQueue:
    """Gestisce coda FIFO per richieste Ollama"""
    
    def __init__(self, max_concurrent=1):
        self.queue = deque()  # FIFO queue
        self.active_requests = {}  # request in processing
        self.lock = threading.Lock()  # Thread-safe operations
        self.max_concurrent = 1  # Ollama single-threaded
    
    def enqueue(self, session_id, data):
        """Aggiunge richiesta alla coda"""
        request_id = self.request_counter
        self.queue.append({
            'session_id': session_id,
            'request_id': request_id,
            'data': data,
            'status': 'queued',
            'enqueued_at': datetime.now()
        })
        return request_id
    
    def get_position(self, request_id):
        """Posizione in coda (1-indexed)"""
        for i, req in enumerate(self.queue):
            if req['request_id'] == request_id:
                return i + 1
        return None
    
    def start_processing(self, request_id):
        """Marca come in processing (lock Ollama)"""
        req = self.queue.pop(0)  # FIFO
        self.active_requests[req['session_id']] = req
        return req
    
    def finish_processing(self, session_id):
        """Rilascia lock Ollama"""
        self.active_requests.pop(session_id)
```

### 2. Session Management

```python
# Flask session per isolare utenti
app.secret_key = 'change-in-production'

_conversation_sessions = {}  # session_id -> history

def get_session_id():
    if 'session_id' not in session:
        session['session_id'] = str(uuid.uuid4())
    return session['session_id']

def get_conversation_history(session_id):
    if session_id not in _conversation_sessions:
        _conversation_sessions[session_id] = []
    return _conversation_sessions[session_id]
```

### 3. Endpoint con Queue

```python
@app.route('/chat/stream', methods=['POST'])
def chat_stream():
    session_id = get_session_id()
    
    # Enqueue request
    request_id = request_queue.enqueue(session_id, {
        'message': user_message
    })
    
    def generate():
        # Invia queue position
        position = request_queue.get_position(request_id)
        if position > 1:
            yield f"data: {json.dumps({'type': 'queue', 'position': position})}\n\n"
        
        # Polling: attendi il turno
        while True:
            if request_queue.get_next_request() == request_id:
                request_queue.start_processing(request_id)
                break
            time.sleep(0.5)
        
        # Processa (abbiamo il lock)
        # ... RAG retrieval
        # ... Ollama streaming
        
        # Rilascia lock
        request_queue.finish_processing(session_id)
    
    return Response(stream_with_context(generate()))
```

## FLUSSO MULTI-USER

### Scenario: 3 utenti simultanei

**t=0s:**
```
User A: Invia query ‚Üí Enqueued #1 ‚Üí Processing immediately
User B: Invia query ‚Üí Enqueued #2 ‚Üí Queue position: 2
User C: Invia query ‚Üí Enqueued #3 ‚Üí Queue position: 3
```

**t=5s:**
```
User A: Processing... (Ollama genera)
User B: UI mostra "In coda: posizione 2"
User C: UI mostra "In coda: posizione 3"
```

**t=35s:**
```
User A: ‚úÖ Done ‚Üí Rilascia lock
User B: Position 2 ‚Üí 1 ‚Üí Processing starts
User C: Position 3 ‚Üí 2
```

**t=70s:**
```
User B: ‚úÖ Done ‚Üí Rilascia lock
User C: Position 2 ‚Üí 1 ‚Üí Processing starts
```

**t=105s:**
```
User C: ‚úÖ Done
Queue empty
```

## FRONTEND INTEGRATION

### API Response Events

```javascript
// SSE event types
{type: 'queue', position: 2, message: 'In coda: posizione 2'}
{type: 'sources', sources: [...]}
{type: 'token', token: 'Il '}
{type: 'done', timestamp: '...'}
{type: 'error', error: '...'}
```

### UI Display

```javascript
onQueue: (data) => {
    // Mostra posizione in coda
    showQueueStatus(`‚è≥ In coda: posizione ${data.position}`);
},

onSources: (sources) => {
    // Nascond queue status, mostra sources
    hideQueueStatus();
    showSources(sources);
},

onToken: (token) => {
    // Rendering progressivo
    appendToken(token);
}
```

### UI Example

```
Prima della risposta:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚è≥ In coda: posizione 2            ‚îÇ
‚îÇ                                    ‚îÇ
‚îÇ Ci sono altri utenti in attesa.   ‚îÇ
‚îÇ Tempo stimato: ~35s                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Durante la risposta:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üìö Sources: [TK3+ 130bar (63%)]   ‚îÇ
‚îÇ                                    ‚îÇ
‚îÇ Il TK3+ 130bar √®...               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## BENEFITS

### 1. No Timeout Errors
- ‚ùå PRIMA: User B timeout dopo 60s (mentre User A processa 35s)
- ‚úÖ DOPO: User B vede queue position, attende senza timeout

### 2. Transparent UX
- User vede posizione in coda
- Tempo attesa stimato (position * 35s avg)
- Professional B2B experience

### 3. Session Isolation
- Ogni utente ha la propria cronologia
- No cross-contamination tra conversazioni
- Cookie-based session management

### 4. Fair Scheduling
- FIFO (First In First Out)
- No priority (tutti utenti uguali)
- Predictable wait times

## SCALABILITY

### Current Setup (Single Ollama Instance)
- Max concurrent: 1
- Queue size: Unlimited
- Throughput: ~100 requests/hour (35s avg)

### Future: Multiple Ollama Instances
```python
request_queue = RequestQueue(max_concurrent=3)  # 3 Ollama workers
```
- Throughput: ~300 requests/hour
- Reduce wait time 3x

### Production Recommendations
1. **Redis Queue:** Sostituisci deque con Redis (persist across restarts)
2. **Load Balancer:** Nginx round-robin su multiple Flask instances
3. **Ollama Pool:** 3-5 Ollama instances su GPU diverse
4. **Priority Queue:** B2B enterprise users > free tier
5. **Rate Limiting:** Max 10 requests/user/hour

## TESTING

### Test Multi-User

```bash
# Terminal 1
curl -X POST http://localhost:5000/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message": "TK3+ per CO2?"}'

# Terminal 2 (simultaneo)
curl -X POST http://localhost:5000/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message": "LC-XP vs LC-XT?"}'

# Terminal 3 (simultaneo)
curl -X POST http://localhost:5000/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message": "TK4 MODBUS register map?"}'
```

**Expected Output:**
```
Terminal 1: Immediate processing
Terminal 2: data: {"type":"queue","position":2}
Terminal 3: data: {"type":"queue","position":3}
```

## IMPLEMENTATION STATUS

### ‚úÖ Implemented
- [x] RequestQueue class
- [x] Session management
- [x] Queue position tracking
- [x] FIFO scheduling
- [x] Thread-safe operations
- [x] SSE queue events

### ‚ö†Ô∏è Pending
- [ ] Frontend queue UI display
- [ ] Estimated wait time calculation
- [ ] Redis persistence (optional)
- [ ] Admin dashboard (queue status)
- [ ] Priority queue levels
- [ ] Rate limiting per user

### üîÆ Future Enhancements
- [ ] Multiple Ollama instances (pool)
- [ ] WebSocket upgrade (bidirectional)
- [ ] Request cancellation
- [ ] Queue analytics (avg wait time, peak hours)
- [ ] Auto-scaling (spawn Ollama instances on demand)

## CONCLUSION

**Queue system:** ‚úÖ ESSENTIAL per B2B deployment  
**Implementation:** ‚úÖ Ready (codice completo in app.py)  
**Testing:** Necessario stress test con 5-10 utenti simultanei  
**Production-ready:** 90% (manca solo UI queue display + test)

**Next step:** Implementare UI queue display nel frontend.
