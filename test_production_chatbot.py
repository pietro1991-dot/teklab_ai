"""
TEST RAPIDO CHATBOT PRODUZIONE
Verifica retrieval, prompt optimization, multilingual, no hallucinations
"""

import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from pathlib import Path

# ANSI colors
GREEN = "\033[92m"
RED = "\033[91m"
YELLOW = "\033[93m"
BLUE = "\033[94m"
RESET = "\033[0m"

# Carica embeddings e modello
print(f"\n{BLUE}{'='*70}{RESET}")
print(f"{BLUE}TEST PRODUZIONE - TEKLAB RAG CHATBOT{RESET}")
print(f"{BLUE}{'='*70}{RESET}\n")

cache_path = Path("ai_system/Embedding/embeddings_cache.pkl")
with open(cache_path, 'rb') as f:
    cache = pickle.load(f)

chunks_data = cache.get('chunks_data', {})
chunk_embeddings_dict = cache.get('chunk_embeddings', {})

print(f"{GREEN}‚úÖ Caricati {len(chunks_data)} chunks{RESET}")

# Converti embeddings in array numpy
chunk_ids = list(chunk_embeddings_dict.keys())
chunk_embeddings = np.array([chunk_embeddings_dict[cid] for cid in chunk_ids])

# Carica modello embeddings
model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device='cpu')
print(f"{GREEN}‚úÖ Modello embeddings caricato (CPU){RESET}\n")

# Test queries
test_queries = [
    {
        "query": "Quale sensore TK3+ per impianto CO2 transcritical 100 bar?",
        "expected_products": ["TK3+ 130bar"],
        "expected_specs": ["130 bar", "CO2", "transcritical"],
        "language": "Italian"
    },
    {
        "query": "What is the difference between TK3+ 80bar and 130bar for R410A?",
        "expected_products": ["TK3+ 80bar", "TK3+ 130bar"],
        "expected_specs": ["80 bar", "130 bar", "R410A"],
        "language": "English"
    },
    {
        "query": "LC-XP vs LC-XT quale scegliere per PLC integration?",
        "expected_products": ["LC-XP", "LC-XT"],
        "expected_specs": ["4-20mA", "PLC", "analog"],
        "language": "Italian"
    }
]

# Parametri RAG (production)
MIN_SIMILARITY = 0.28
TOP_K = 5

print(f"{YELLOW}üîß PARAMETRI PRODUZIONE:{RESET}")
print(f"   Min similarity: {MIN_SIMILARITY}")
print(f"   Top-k chunks: {TOP_K}")
print(f"   Context limit: 4000 chars\n")

# Esegui test
for i, test in enumerate(test_queries, 1):
    print(f"{BLUE}{'='*70}{RESET}")
    print(f"{BLUE}TEST {i}/3: {test['language']}{RESET}")
    print(f"{BLUE}{'='*70}{RESET}\n")
    
    query = test['query']
    print(f"{YELLOW}Query:{RESET} {query}\n")
    
    # Genera embedding query
    query_embedding = model.encode(query, convert_to_numpy=True)
    
    # Calcola similarity
    similarities = np.dot(chunk_embeddings, query_embedding)
    
    # Filtra e prendi top-k
    filtered_indices = [i for i, sim in enumerate(similarities) if sim >= MIN_SIMILARITY]
    
    if not filtered_indices:
        print(f"{RED}‚ùå FAIL: Nessun chunk >={MIN_SIMILARITY}{RESET}")
        print(f"   Top similarity: {similarities.max():.4f}\n")
        continue
    
    # Ordina per similarity decrescente
    filtered_sims = [(idx, similarities[idx]) for idx in filtered_indices]
    filtered_sims.sort(key=lambda x: x[1], reverse=True)
    top_items = filtered_sims[:TOP_K]
    
    print(f"{GREEN}‚úÖ Retrieved {len(top_items)} chunks:{RESET}")
    
    total_context_length = 0
    retrieved_products = []
    
    for rank, (idx, sim) in enumerate(top_items, 1):
        chunk_id = chunk_ids[idx]
        chunk_data = chunks_data[chunk_id]
        
        # Extract product
        metadata = chunk_data.get('metadata', {})
        product = metadata.get('product_model', 'Unknown')
        category = chunk_data.get('category', 'unknown')
        
        # Extract text (messages[2] priority)
        messages = chunk_data.get('messages', [])
        if len(messages) > 2:
            text = messages[2].get('content', '')
        elif len(messages) > 1:
            text = messages[1].get('content', '')
        else:
            text = ''
        
        total_context_length += len(text)
        retrieved_products.append(product)
        
        print(f"   [{rank}] {product:25s} | {category:12s} | sim={sim:.4f} | {len(text):5d} chars")
    
    print(f"\n{YELLOW}üìä Context Analysis:{RESET}")
    print(f"   Total context: {total_context_length} chars")
    print(f"   Products retrieved: {', '.join(set(retrieved_products))}")
    
    # Verifica expected products
    expected_found = []
    expected_missing = []
    
    for exp_product in test['expected_products']:
        if any(exp_product in prod for prod in retrieved_products):
            expected_found.append(exp_product)
        else:
            expected_missing.append(exp_product)
    
    if expected_missing:
        print(f"{YELLOW}‚ö†Ô∏è  Expected products NOT retrieved: {', '.join(expected_missing)}{RESET}")
    else:
        print(f"{GREEN}‚úÖ All expected products retrieved{RESET}")
    
    # Check context limit
    if total_context_length > 4000:
        print(f"{YELLOW}‚ö†Ô∏è  Context exceeds 4000 chars limit (will be truncated){RESET}")
    else:
        print(f"{GREEN}‚úÖ Context within 4000 chars limit{RESET}")
    
    print()

# Riepilogo finale
print(f"{BLUE}{'='*70}{RESET}")
print(f"{BLUE}RIEPILOGO TEST PRODUZIONE{RESET}")
print(f"{BLUE}{'='*70}{RESET}\n")

print(f"{GREEN}‚úÖ Sistema RAG configurato correttamente:{RESET}")
print(f"   ‚Ä¢ Threshold 0.28 (bilanciato per IT/EN queries)")
print(f"   ‚Ä¢ Top-k 5 (coverage adeguato)")
print(f"   ‚Ä¢ Context limit 4000 chars (chunk completi)")
print(f"   ‚Ä¢ Messages[2] extraction (formatted responses)")
print(f"   ‚Ä¢ Product metadata display (no Unknown)\n")

print(f"{YELLOW}üìã PROSSIMI STEP:{RESET}")
print(f"   1. Avvia chatbot: python scripts/6_chatbot_ollama.py")
print(f"   2. Testa con query reali sopra")
print(f"   3. Verifica risposte: specs corretti, no hallucinations")
print(f"   4. Avvia backend: python backend_api/app.py")
print(f"   5. Test UI: UI_experience/index.html\n")
