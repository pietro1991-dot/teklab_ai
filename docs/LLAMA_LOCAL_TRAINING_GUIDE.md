# ğŸ  Guida: Llama in Locale + Training con Conversazioni Utenti

Questa guida spiega come:
1. **Scaricare Llama in locale** (funziona offline)
2. **Fine-tunare con chunks RAG + conversazioni utenti reali**

---

## ğŸ¯ Workflow Completo

```
1. Download Llama â†’ 2. Usa in locale â†’ 3. Raccogli conversazioni â†’ 4. Fine-tuning â†’ 5. Migliora nel tempo
```

---

## 1ï¸âƒ£ Scaricare Llama in Locale (UNA VOLTA)

### Step 1: Login HuggingFace

```bash
# Installa CLI
pip install huggingface-hub

# Login (serve token da: https://huggingface.co/settings/tokens)
huggingface-cli login
```

### Step 2: Accetta License Llama

Vai su https://huggingface.co/meta-llama/Llama-2-7b-chat-hf e clicca **"Request Access"**

### Step 3: Scarica Modello

```bash
cd ai_system

# Scarica Llama 2 7B chat (default - ~13GB)
python download_llama.py

# Oppure Llama 3 8B
python download_llama.py --model meta-llama/Meta-Llama-3-8B-Instruct

# Con verifica automatica
python download_llama.py --verify
```

**Tempo stimato**: 10-30 minuti (dipende da connessione)  
**Spazio richiesto**: ~13GB per Llama 2 7B

### Step 4: Verifica Download

```bash
# Controlla che esista
dir ai_system\models\Llama-2-7b-chat-hf

# Dovresti vedere:
# - config.json
# - tokenizer_config.json
# - pytorch_model-*.bin (o model.safetensors)
```

âœ… **Ora il modello Ã¨ LOCALE e funziona OFFLINE!**

---

## 2ï¸âƒ£ Usare Llama Locale nel Chatbot

Il sistema **rileva automaticamente** il modello locale:

```bash
# Avvia chatbot (usa automaticamente modello locale se disponibile)
python BOT\chatbot_llama.py
```

Output atteso:
```
âœ… Uso modello LOCALE: ai_system/models/Llama-2-7b-chat-hf
ğŸ¦™ Caricamento Llama RAG Model...
```

**Nessuna connessione internet richiesta dopo download!**

---

## 3ï¸âƒ£ Raccogliere Conversazioni Utenti

Il chatbot **salva automaticamente** tutte le conversazioni in:
```
ai_system/training_data/conversations/YYYY-MM-DD/
```

Ogni conversazione include:
- Query utente
- Risposta assistente
- Chunks RAG usati
- Timestamp
- Modello usato

**Non fare nulla, succede automaticamente mentre usi il chatbot!**

---

## 4ï¸âƒ£ Creare Dataset da Conversazioni + RAG

Quando hai raccolto abbastanza conversazioni (>10), crea il training dataset:

```bash
cd ai_system

# Crea dataset combinando:
# - Conversazioni utenti reali
# - Chunks RAG esistenti
python src\training\create_training_dataset.py
```

Questo script:
1. âœ… Carica **tutte le conversazioni salvate**
2. âœ… Carica **chunks RAG dalle Fonti**
3. âœ… Crea sample sintetici (se pochi dati utenti)
4. âœ… Split in train/validation/test (80/10/10)
5. âœ… Salva in `src/training/training_dataset/`

Output:
```
âœ… Dataset creato!
   â€¢ Train: 150 samples
   â€¢ Validation: 20 samples
   â€¢ Test: 20 samples
```

---

## 5ï¸âƒ£ Fine-Tuning con Dati Reali

Ora fine-tuna Llama con il dataset personalizzato:

```bash
cd ai_system

# Training QLoRA (GPU 6GB+)
python src\training\train_llama_rag.py --config llama-qlora --epochs 3

# Oppure specifica modello locale
python src\training\train_llama_rag.py --model-name "ai_system/models/Llama-2-7b-chat-hf" --epochs 5
```

**Cosa impara Llama:**
- âœ… Come rispondere alle **domande reali** degli utenti
- âœ… Stile e tono dalle **risposte salvate**
- âœ… Uso dei **chunks RAG specifici** del tuo dominio
- âœ… Pattern conversazionali **personalizzati**

**Tempo**: 1-3 ore (dipende da GPU e dataset size)

---

## 6ï¸âƒ£ Miglioramento Continuo

### Workflow Iterativo

```
Usa chatbot â†’ Conversazioni salvate â†’ Rigenera dataset â†’ Fine-tune â†’ Chatbot migliorato â†’ Ripeti
```

### Ogni N giorni:

```bash
# 1. Rigenera dataset (include nuove conversazioni)
python src\training\create_training_dataset.py

# 2. Fine-tune con dati aggiornati
python src\training\train_llama_rag.py --config llama-qlora --epochs 2

# 3. Chatbot usa automaticamente ultimo checkpoint
python BOT\chatbot_llama.py
```

---

## ğŸ“Š Vantaggi Sistema Locale + Conversazioni

| Feature | Beneficio |
|---------|-----------|
| ğŸ  **Modello locale** | Funziona offline, nessun costo API |
| ğŸ’¬ **Conversazioni reali** | Impara da interazioni vere degli utenti |
| ğŸ¯ **Fine-tuning personalizzato** | Si adatta al tuo dominio specifico |
| ğŸ”„ **Miglioramento continuo** | PiÃ¹ usi il chatbot, piÃ¹ diventa bravo |
| ğŸ“ˆ **Apprendimento incrementale** | Ogni sessione migliora il modello |

---

## ğŸ”§ Esempi Pratici

### Esempio 1: Setup Completo da Zero

```bash
# 1. Download Llama
cd ai_system
python download_llama.py --verify

# 2. Usa chatbot e fai 20+ conversazioni
cd ..
python BOT\chatbot_llama.py

# 3. Crea dataset
cd ai_system
python src\training\create_training_dataset.py

# 4. Fine-tune
python src\training\train_llama_rag.py --config llama-qlora --epochs 3

# 5. Usa modello migliorato
cd ..
python BOT\chatbot_llama.py
```

### Esempio 2: Update Settimanale

```bash
# Ogni lunedÃ¬, aggiorna il modello con nuove conversazioni
cd ai_system

# Rigenera dataset (include conversazioni della settimana)
python src\training\create_training_dataset.py

# Fine-tune veloce (2 epoch sufficienti per update)
python src\training\train_llama_rag.py --config llama-qlora --epochs 2

# Test
cd ..
python BOT\chatbot_llama.py
```

---

## ğŸ“ Struttura File

```
spirituality.ai/
â”œâ”€â”€ ai_system/
â”‚   â”œâ”€â”€ models/                          # Modelli Llama locali
â”‚   â”‚   â””â”€â”€ Llama-2-7b-chat-hf/         # Scaricato da HuggingFace
â”‚   â”‚       â”œâ”€â”€ config.json
â”‚   â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚       â””â”€â”€ pytorch_model*.bin
â”‚   â”œâ”€â”€ training_data/
â”‚   â”‚   â””â”€â”€ conversations/               # Conversazioni salvate automaticamente
â”‚   â”‚       â””â”€â”€ 2025-10-30/
â”‚   â”‚           â”œâ”€â”€ session1.json
â”‚   â”‚           â”œâ”€â”€ session2.json
â”‚   â”‚           â””â”€â”€ daily_aggregate.jsonl
â”‚   â”œâ”€â”€ src/training/
â”‚   â”‚   â”œâ”€â”€ create_training_dataset.py   # Genera dataset da conversazioni
â”‚   â”‚   â”œâ”€â”€ train_llama_rag.py          # Training script
â”‚   â”‚   â””â”€â”€ training_dataset/            # Dataset generato
â”‚   â”‚       â”œâ”€â”€ train_data.json
â”‚   â”‚       â”œâ”€â”€ validation_data.json
â”‚   â”‚       â””â”€â”€ test_data.json
â”‚   â”œâ”€â”€ checkpoints/                     # Modelli fine-tunati
â”‚   â”‚   â””â”€â”€ llama_rag_20251030/
â”‚   â”‚       â””â”€â”€ best_model/              # Usato automaticamente
â”‚   â””â”€â”€ download_llama.py                # Script download
â””â”€â”€ BOT/
    â””â”€â”€ chatbot_llama.py                 # Entry point
```

---

## ğŸ“ Best Practices

### âœ… DO

- **Usa modello locale** dopo primo download (funziona offline)
- **Fai molte conversazioni** prima di fine-tunare (minimo 20-30)
- **Fine-tuna regolarmente** (es. ogni settimana) con nuove conversazioni
- **Usa QLoRA** per risparmiare VRAM (funziona su GPU 6GB)
- **Monitora quality** delle risposte e ri-fine-tuna se necessario

### âŒ DON'T

- Non fine-tunare con <10 conversazioni (troppo pochi dati)
- Non usare sempre HuggingFace online (scarica una volta, poi locale)
- Non cancellare cartella `training_data/conversations/` (contiene dati preziosi!)
- Non fare over-fitting (max 3-5 epoch per update)

---

## ğŸ” Monitoring Quality

### Controlla miglioramenti nel tempo

```bash
# Visualizza statistiche conversazioni
python ai_system\src\training\analyze_conversations.py

# Output:
# Total conversations: 150
# Average response length: 250 words
# Most common topics: meditazione (45), chakra (30), ...
```

### Test qualitÃ  modello

Fai le stesse domande prima e dopo fine-tuning:
```python
# Prima del fine-tuning
"Cos'Ã¨ la meditazione?"
â†’ Risposta generica

# Dopo fine-tuning con conversazioni
"Cos'Ã¨ la meditazione?"
â†’ Risposta personalizzata con terminologia specifica dei tuoi chunks
```

---

## ğŸ†˜ Troubleshooting

### âŒ "Model not found in local directory"
```bash
# Scarica modello
python ai_system\download_llama.py --verify
```

### âŒ "No conversations found"
```bash
# Usa chatbot per creare conversazioni
python BOT\chatbot_llama.py
# Fai almeno 10-20 domande/risposte
```

### âŒ "CUDA out of memory" durante training
```bash
# Usa QLoRA con batch size minimo
python src\training\train_llama_rag.py --config llama-qlora --batch-size 1 --accumulation-steps 32
```

---

## ğŸ“ˆ Roadmap Miglioramenti

### Fase 1: Setup Iniziale (Week 1)
- âœ… Download Llama locale
- âœ… Prime 50 conversazioni
- âœ… Primo fine-tuning

### Fase 2: Raccolta Dati (Week 2-4)
- ğŸ“Š 100+ conversazioni
- ğŸ”„ Fine-tuning settimanale
- ğŸ“ˆ Monitoring quality

### Fase 3: Produzione (Week 5+)
- ğŸš€ Modello stabile
- ğŸ”„ Update automatici
- ğŸ“Š Analytics avanzati

---

## ğŸ¯ Riepilogo Comandi Essenziali

```bash
# Setup iniziale (UNA VOLTA)
huggingface-cli login
python ai_system\download_llama.py --verify

# Uso quotidiano
python BOT\chatbot_llama.py

# Update settimanale (dopo 20+ nuove conversazioni)
python ai_system\src\training\create_training_dataset.py
python ai_system\src\training\train_llama_rag.py --config llama-qlora --epochs 2
```

---

**Creato**: Ottobre 2025  
**Versione**: 2.0 (Locale + Conversazioni)
